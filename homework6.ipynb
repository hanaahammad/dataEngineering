{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "323abc5c",
   "metadata": {},
   "source": [
    "# Question 1: Redpanda version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60408b57",
   "metadata": {},
   "source": [
    "Now let's find out the version of redpandas.\n",
    "\n",
    "For that, check the output of the command rpk help inside the container. The name of the container is redpanda-1.\n",
    "\n",
    "Find out what you need to execute based on the help output.\n",
    "\n",
    "What's the version, based on the output of the command you executed? (copy the entire version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cebc88",
   "metadata": {},
   "source": [
    "rpk version v23.3.6 (rev 78642d6969)\n",
    "redpanda@bf0f76ef0fd5:/$ rpk version\n",
    "Version:     v23.3.6\n",
    "Git ref:     78642d6969\n",
    "Build date:  2024-02-21T20:04:31Z\n",
    "OS/Arch:     linux/amd64\n",
    "Go version:  go1.21.3\n",
    "\n",
    "Redpanda Cluster\n",
    "  node-0  v23.3.6 â€“ 78642d69697481ab985a808d95d7943811bfeca9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badf5de5",
   "metadata": {},
   "source": [
    "# Question 2. Creating a topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6620db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "edpanda@bf0f76ef0fd5:/$ rpk topic create test-topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a5325e",
   "metadata": {},
   "source": [
    "edpanda@bf0f76ef0fd5:/$ rpk topic create test-topic\n",
    "\n",
    "TOPIC       STATUS\n",
    "test-topic  OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306f48fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12c0330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa789f71",
   "metadata": {},
   "source": [
    "# Question 3. Connecting to the Kafka server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b1ff8e",
   "metadata": {},
   "source": [
    "We need to make sure we can connect to the server, so later we can send some data to its topics\n",
    "\n",
    "First, let's install the kafka connector (up to you if you want to have a separate virtual environment for that)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da13338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python in /home/hhammad/miniconda3/envs/stenv/lib/python3.10/site-packages (2.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031cf334",
   "metadata": {},
   "source": [
    "Let's try to connect to our server:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "495064ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time \n",
    "\n",
    "from kafka import KafkaProducer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf0f2c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def json_serializer(data):\n",
    "    return json.dumps(data).encode('utf-8')\n",
    "\n",
    "server = 'localhost:9092'\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[server],\n",
    "    value_serializer=json_serializer\n",
    ")\n",
    "\n",
    "producer.bootstrap_connected()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0c08e2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8fcbbe9",
   "metadata": {},
   "source": [
    "Provided that you can connect to the server, what's the output of the last command?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557a3fe0",
   "metadata": {},
   "source": [
    "True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6caac91",
   "metadata": {},
   "source": [
    "# Question 4. Sending data to the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9975e303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: {'number': 0}\n",
      "Sent: {'number': 1}\n",
      "Sent: {'number': 2}\n",
      "Sent: {'number': 3}\n",
      "Sent: {'number': 4}\n",
      "Sent: {'number': 5}\n",
      "Sent: {'number': 6}\n",
      "Sent: {'number': 7}\n",
      "Sent: {'number': 8}\n",
      "Sent: {'number': 9}\n",
      "took 0.50 seconds\n",
      "took 0.50 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "topic_name = 'test-topic'\n",
    "\n",
    "for i in range(10):\n",
    "    message = {'number': i}\n",
    "    producer.send(topic_name, value=message)\n",
    "    print(f\"Sent: {message}\")\n",
    "    time.sleep(0.05)\n",
    "\n",
    "t2= time.time()\n",
    "producer.flush()\n",
    "\n",
    "t1 = time.time()\n",
    "print(f'took {(t1 - t0):.2f} seconds')\n",
    "print(f'took {(t2 - t0):.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c762e2",
   "metadata": {},
   "source": [
    "## Sending the taxi data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "729c248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d3df8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d15b27e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3630350/16377989.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_green = pd.read_csv('../green_tripdata_2019-10.csv')\n"
     ]
    }
   ],
   "source": [
    "df_green = pd.read_csv('../green_tripdata_2019-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93585dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(476386, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47de9de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = df_green[['lpep_pickup_datetime', 'lpep_dropoff_datetime',\n",
    "       'PULocationID', 'DOLocationID',\n",
    "       'passenger_count', 'trip_distance',\n",
    "       'tip_amount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d908fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['lpep_pickup_datetime', 'lpep_dropoff_datetime', 'PULocationID',\n",
       "       'DOLocationID', 'passenger_count', 'trip_distance', 'tip_amount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cef186fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lpep_pickup_datetime': '2019-10-01 00:26:02', 'lpep_dropoff_datetime': '2019-10-01 00:39:58', 'PULocationID': 112, 'DOLocationID': 196, 'passenger_count': 1.0, 'trip_distance': 5.88, 'tip_amount': 0.0}\n"
     ]
    }
   ],
   "source": [
    "for row in df_green.itertuples(index=False):\n",
    "    row_dict = {col: getattr(row, col) for col in row._fields}\n",
    "    print(row_dict)\n",
    "    break\n",
    "\n",
    "    # TODO implement sending the data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05fe271c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(476386, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c9818f",
   "metadata": {},
   "source": [
    "# Question 5: Sending the Trip Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c5a3fb",
   "metadata": {},
   "source": [
    "Create a topic green-trips and send the data there\n",
    "How much time in seconds did it take? (You can round it to a whole number)\n",
    "Make sure you don't include sleeps in your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309b970d",
   "metadata": {},
   "source": [
    "rpk topic create green-trips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f989eda",
   "metadata": {},
   "source": [
    "redpanda@bf0f76ef0fd5:/$ rpk topic create green-tripsps\n",
    "TOPIC        STATUS\n",
    "green-trips  OK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9822210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 31.89 seconds\n",
      "took 31.88 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "topic_name = 'green-trips'\n",
    "'''\n",
    "for i in range(10):\n",
    "    message = {'number': i}\n",
    "    producer.send(topic_name, value=message)\n",
    "    print(f\"Sent: {message}\")\n",
    "    time.sleep(0.05)\n",
    "'''\n",
    "for row in df_green.itertuples(index=False):\n",
    "    row_dict = {col: getattr(row, col) for col in row._fields}\n",
    "    producer.send(topic_name, value=row_dict)\n",
    "    #print(row_dict)\n",
    "    #break\n",
    "    \n",
    "t2= time.time()\n",
    "producer.flush()\n",
    "\n",
    "t1 = time.time()\n",
    "print(f'took {(t1 - t0):.2f} seconds')\n",
    "print(f'took {(t2 - t0):.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43e1e6",
   "metadata": {},
   "source": [
    "## Creating the PySpark consumer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb0fce6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/05 19:29:53 WARN Utils: Your hostname, loula resolves to a loopback address: 127.0.1.1; using 192.168.122.1 instead (on interface virbr0)\n",
      "24/04/05 19:29:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/hhammad/miniconda3/envs/stenv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/hhammad/.ivy2/cache\n",
      "The jars for the packages stored in: /home/hhammad/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a19d8e5e-fc47-4158-b0ad-f27d1e435a08;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 356ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a19d8e5e-fc47-4158-b0ad-f27d1e435a08\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/9ms)\n",
      "24/04/05 19:29:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/05 19:29:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "pyspark_version = pyspark.__version__\n",
    "kafka_jar_package = f\"org.apache.spark:spark-sql-kafka-0-10_2.12:{pyspark_version}\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"GreenTripsConsumer\") \\\n",
    "    .config(\"spark.jars.packages\", kafka_jar_package) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bde32f",
   "metadata": {},
   "source": [
    "### Now we can connect to the stream:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e9d2f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"green-trips\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5239670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55b6508f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/05 19:30:11 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-7e4b70ba-98c7-48bf-8cb0-87e46031c122. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/05 19:30:11 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/04/05 19:30:12 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(key=None, value=bytearray(b'{\"VendorID\": 2.0, \"lpep_pickup_datetime\": \"2019-10-01 00:26:02\", \"lpep_dropoff_datetime\": \"2019-10-01 00:39:58\", \"store_and_fwd_flag\": \"N\", \"RatecodeID\": 1.0, \"PULocationID\": 112, \"DOLocationID\": 196, \"passenger_count\": 1.0, \"trip_distance\": 5.88, \"fare_amount\": 18.0, \"extra\": 0.5, \"mta_tax\": 0.5, \"tip_amount\": 0.0, \"tolls_amount\": 0.0, \"ehail_fee\": NaN, \"improvement_surcharge\": 0.3, \"total_amount\": 19.3, \"payment_type\": 2.0, \"trip_type\": 1.0, \"congestion_surcharge\": 0.0}'), topic='green-trips', partition=0, offset=0, timestamp=datetime.datetime(2024, 4, 5, 17, 15, 36, 186000), timestampType=0)\n"
     ]
    }
   ],
   "source": [
    "def peek(mini_batch, batch_id):\n",
    "    first_row = mini_batch.take(1)\n",
    "\n",
    "    if first_row:\n",
    "        print(first_row[0])\n",
    "\n",
    "query = green_stream.writeStream.foreachBatch(peek).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f88e99",
   "metadata": {},
   "source": [
    "# Question 6. Parsing the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814aa45a",
   "metadata": {},
   "source": [
    "The data is JSON, but currently it's in binary format. We need to parse it and turn it into a streaming dataframe with proper columns.\n",
    "\n",
    "Similarly to PySpark, we define the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7bf7b2be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['lpep_pickup_datetime', 'lpep_dropoff_datetime', 'PULocationID',\n",
       "       'DOLocationID', 'passenger_count', 'trip_distance', 'tip_amount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "46937c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "schema = types.StructType() \\\n",
    "    .add(\"lpep_pickup_datetime\", types.StringType()) \\\n",
    "    .add(\"lpep_dropoff_datetime\", types.StringType()) \\\n",
    "    .add(\"PULocationID\", types.IntegerType()) \\\n",
    "    .add(\"DOLocationID\", types.IntegerType()) \\\n",
    "    .add(\"passenger_count\", types.DoubleType()) \\\n",
    "    .add(\"trip_distance\", types.DoubleType()) \\\n",
    "    .add(\"tip_amount\", types.DoubleType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43702dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "180df132",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "green_stream = green_stream \\\n",
    "  .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"data\")) \\\n",
    "  .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eadc17e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/05 19:17:52 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-5ea88144-b59c-43cd-84ef-4b8856f86f33. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/05 19:17:52 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/04/05 19:17:52 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(lpep_pickup_datetime='2019-10-01 00:26:02', lpep_dropoff_datetime='2019-10-01 00:39:58', PULocationID=112, DOLocationID=196, passenger_count=1.0, trip_distance=5.88, tip_amount=0.0)\n"
     ]
    }
   ],
   "source": [
    "query = green_stream.writeStream.foreachBatch(peek).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55748bc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'path' is not specified.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3497302/1336938743.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgreen_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/stenv/lib/python3.10/site-packages/pyspark/sql/streaming/readwriter.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/stenv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'path' is not specified."
     ]
    }
   ],
   "source": [
    "query = green_stream.writeStream.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "963b895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3256520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eeb0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_stream = green_stream \\\n",
    "  .select(F.from_json(F.col('value').cast('string'), schema).alias(\"data\")) \\\n",
    "  .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9571d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "topic_name = 'green-trips'\n",
    "\n",
    "df = spark.readStream \\\n",
    "     .format(\"kafka\") \\\n",
    "     .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "     .option(\"subscribe\", topic_name) \\\n",
    "     .option(\"startingOffsets\", \"earliest\") \\\n",
    "     .load() \\\n",
    "     .select(F.col('key').cast('string'),\n",
    "             F.from_json(F.col(\"value\").cast(\"string\"), schema).alias('value'),\n",
    "             F.col('topic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8cd38c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9c322c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9148c316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- lpep_pickup_datetime: string (nullable = true)\n",
      " |    |-- lpep_dropoff_datetime: string (nullable = true)\n",
      " |    |-- PULocationID: integer (nullable = true)\n",
      " |    |-- DOLocationID: integer (nullable = true)\n",
      " |    |-- passenger_count: double (nullable = true)\n",
      " |    |-- trip_distance: double (nullable = true)\n",
      " |    |-- tip_amount: double (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195ab536",
   "metadata": {},
   "source": [
    "# Question 7: Most popular destination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71066f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame representing the stream of input lines from connection to localhost:9999\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Split the lines into words\n",
    "words = lines.select(\n",
    "   explode(\n",
    "       split(lines.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "# Generate running word count\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "\n",
    "popular_destinations="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3c15f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start running the query that prints the running counts to the console\n",
    "query = wordCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "072d1dbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'popular_destinations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3630350/3300049491.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopular_destinations\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"console\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"truncate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"false\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'popular_destinations' is not defined"
     ]
    }
   ],
   "source": [
    "query = popular_destinations \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab575c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
